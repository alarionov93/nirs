Отчет по практике.


Введение
Цель - разарботка генератора текстов на базе цепей Маркова.

Задачи

1. Изучение литературы
2. Поиск алгоримических решений
3. Реализация программы

Изучение литературы
Введение в теорию марковских цепей
Цепью Маркова называют такую последовательность случайных событий, в которой вероятность каждого события зависит только от состояния, в котором процесс находится в текущий момент и не зависит от более ранних состояний. Конечная дискретная цепь определяется:

множеством состояний S = {s1, …, sn}, событием является переход из одного состояния в другое в результате случайного испытания
вектором начальных вероятностей (начальным распределением) p(0) = {p(0)(1),…, p(0)(n)}, определяющим вероятности p(0)(i) того, что в начальный момент времени t = 0 процесс находился в состоянии si
матрицей переходных вероятностей P = {pij}, характеризующей вероятность перехода процесса с текущим состоянием si в следующее состояние sj, при этом сумма вероятностей переходов из одного состояния равна 1:
∑j=1…n  pij = 1
Пример матрицы переходных вероятностей с множеством состояний S = {S1, …, S5}, вектором начальных вероятностей p(0) = {1, 0, 0, 0, 0} изображен на рисунке 1.

// вставить рисунок
Рис.1 Пример матрицы переходных вероятностей

С помощью вектора начальных вероятностей и матрицы переходов можно вычислить стохастический вектор p(n) — вектор, составленный из вероятностей p(n)(i) того, что процесс окажется в состоянии i в момент времени n. Получить p(n) можно с помощью формулы:

p(n) = p(0)×P n
Векторы p(n) при росте n в некоторых случаях стабилизируются — сходятся к некоторому вероятностному вектору ρ, который можно назвать стационарным распределением цепи. Стационарность проявляется в том, что взяв p(0) = ρ, мы получим p(n) = ρ для любого n.
Простейший критерий, который гарантирует сходимость к стационарному распределению, выглядит следующим образом: если все элементы матрицы переходных вероятностей P положительны, то при n, стремящемуся к бесконечности, вектор p(n) стремится к вектору ρ, являющемуся единственным решением системы вида 
p × P = p.
Также можно показать, что если при каком-нибудь положительном значении n все элементы матрицы P n положительны, тогда вектор p(n) все-равно будет стабилизироваться.
Доказательство этих утверждений есть в [1] в подробном виде.

Марковская цепь изображается в виде графа переходов, вершины которого соответствуют состояниям цепи, а дуги — переходам между ними. Вес дуги (i, j), связывающей вершины si и sj будет равен вероятности pi(j) перехода из первого состояния во второе. Граф, соответствующий матрице, изображенной выше, представлен на рисунке 2.

// вставить рисунок
Рис.2 Граф, соответствующий матрице переходных вероятностей.
При рассмотрении цепей Маркова нас может интересовать поведение системы на коротком отрезке времени. В таком случае абсолютные вероятности вычисляются с помощью формул из предыдущего раздела. Однако более важно изучить поведение системы на большом интервале времени, когда число переходов стремится к бесконечности. Далее вводятся определения состояний марковских цепей, которые необходимы для изучения поведения системы в долгосрочной перспективе.
Марковские цепи классифицируются в зависимости от возможности перехода из одних состояний в другие. 
Группы состояний марковской цепи (подмножества вершин графа переходов), которым соответствуют тупиковые вершины диаграммы порядка графа переходов, называются эргодическими классами цепи. Если рассмотреть граф, изображенный выше, то видно, что в нем 1 эргодический класс M1 = {S5}, достижимый из компоненты сильной связности, соответствующей подмножеству вершин M2 = {S1, S2, S3, S4}. Состояния, которые находятся в эргодических классах, называются существенными, а остальные — несущественными (хотя такие названия плохо согласуются со здравым смыслом). Поглощающее состояние si является частным случаем эргодического класса. Тогда попав в такое состояние, процесс прекратится. Для Si будет верно pii = 1, т.е. в графе переходов из него будет исходить только одно ребро — петля.

Поглощающие марковские цепи используются в качестве временных моделей программ и вычислительных процессов. При моделировании программы состояния цепи отождествляются с блоками программы, а матрица переходных вероятностей определяет порядок переходов между блоками, зависящий от структуры программы и распределения исходных данных, значения которых влияют на развитие вычислительного процесса. В результате представления программы поглощающей цепью удается вычислить число обращений к блокам программы и время выполнения программы, оцениваемое средними значениями, дисперсиями и при необходимости — распределениями. Используя в дальнейшем эту статистику, можно оптимизировать код программы — применять низкоуровневые методы для ускорения критических частей программы. Подобный метод называется профилированием кода.

Например, в алгоритме Дейкстры присутствуют следующие состояния цепи:

vertex (v), извлечение новой вершины из очереди с приоритетами, переход только в состояние b;
begin (b), начало цикла перебора исходящих дуг для процедуры ослабления;
analysis (a), анализ следующей дуги, возможен переход к a, d, или e;
decrease (d), уменьшение оценки для некоторой вершины графа, переход к a;
end (e), завершение работы цикла, переход к следующей вершине.
Остается задать вероятности переходом между вершинами, и можно изучать продолжительности переходов между вершинами, вероятности попадания в различные состояния и другие средние характеристики процесса.
Аналогично, вычислительный процесс, который сводится к обращениям за ресурсами системы в порядке, определяемом программой, можно представить поглощающей марковской цепью, состояния которой соответствуют использованию ресурсов системы – процессора, памяти и периферийных устройств, переходные вероятности отображают порядок обращения к различным ресурсам. Благодаря этому вычислительный процесс представляется в форме, удобной для анализа его характеристик.

Цепь Маркова называется неприводимой, если любое состояние Sj может быть достигнуто из любого другого состояния Si за конечное число переходов. В этом случае все состояния цепи называются сообщающимися, а граф переходов является компонентой сильной связности. Процесс, порождаемый эргодической цепью, начавшись в некотором состоянии, никогда не завершается, а последовательно переходит из одного состояния в другое, попадая в различные состояния с разной частотой, зависящей от переходных вероятностей. Поэтому основная характеристика эргодической цепи – 
вероятности пребывания процесса в состояниях Sj, j = 1,…, n, доля времени, которую процесс проводит в каждом из состояний. Неприводимые цепи используются в качестве моделей надежности систем. Действительно, при отказе ресурса, который процесс использует очень часто, работоспособность всей системы окажется под угрозой. В таком случае дублирование такого критического ресурса может помочь избежать отказов. При этом состояния системы, различающиеся составом исправного и отказавшего оборудования, трактуются как состояния цепи, переходы между которыми связаны с отказами и восстановлением устройств и изменением связей между ними, проводимой для сохранения работоспособности системы. Оценки характеристик неприводимой цепи дают представление о надежности поведения системы в целом. Также такие цепи могут быть моделями взаимодействия устройств с задачами, поступающими на обработку.
// Подвести к теме работы - Моделирование сочетаний слов в тексте
Рассмотрим текст, состоящий из слов w. Представим процесс, в котором состояниями являются слова, так что когда он находится в состоянии (Si) система переходит в состояние (sj) согласно матрице переходных вероятностей. Прежде всего, надо «обучить» систему: подать на вход достаточно большой текст для оценки переходных вероятностей. А затем можно строить траектории марковской цепи. Увеличение смысловой нагрузки текста, построенного при помощи алгоритма цепей Маркова возможно только при увеличении порядка, где состоянием является не одно слово, а множества с большей мощностью — пары (u, v), тройки (u, v, w) и т.д. Причем что в цепях первого, что пятого порядка, смысла будет еще немного. Смысл начнет появляться при увеличении размерности порядка как минимум до среднего количества слов в типовой фразе исходного текста. Но таким путем двигаться нельзя, потому, что рост смысловой нагрузки текста в цепях Маркова высоких порядков происходит значительно медленнее, чем падение уникальности текста. А текст, построенный на марковских цепях, к примеру, тридцатого порядка, все еще будет не настолько осмысленным, чтобы представлять интерес для человека, но уже достаточно схожим с оригинальным текстом, к тому же число состояний в такой цепи будет потрясающим.
Эта технология сейчас очень широко применяется (к сожалению) в Интернете для создания контента веб-страниц. Люди, желающие увеличить трафик на свой сайт и повысить его рейтинг в поисковых системах, стремятся поместить на свои страницы как можно больше ключевых слов для поиска. Но поисковики используют алгоритмы, которые умеют отличать реальный текст от бессвязного нагромождения ключевых слов. Тогда, чтобы обмануть поисковики используют тексты, созданные генератором на основе марковской цепи. Есть, конечно, и положительные примеры использования цепей Маркова для работы с текстом, их применяют при определении авторства, анализе подлинности текстов.
Поиск алгоримических решений

Для решения задачи генерации текста необходимо выстроить слова в определенном порядке. Определить порядок можно следующим образом: построить фразы, наиболее вероятные для русского языка. Эту вероятность можно задать формально как вероятность возникновения последовательности слов в неком наборе текстов. К примеру, вероятность фразы «счастье есть удовольствие без раскаяния» можно вычислить как произведение вероятностей каждого из слов этой фразы: 

P = P(счастье) P(есть|счастье) P(удовольствие|счастье есть) P(без|счастье есть удовольствие) P(раскаяния|счастье есть удовольствие без)


Рассчитать вероятность P(счастье) можно так: нужно посчитать сколько раз это слово встретилось в тексте и поделить это значение на общее число слов. Но рассчитать вероятность P(раскаяния|счастье есть удовольствие без) не так просто. Эта задача может быть упрощена, если принять, что вероятность слова в тексте зависит только от предыдущего слова. Тогда формула для расчета фразы примет следующий вид:

P = P(счастье) P(есть|счастье) P(удовольствие|есть) P(без|удовольствие) P(раскаяния|без)

Для того, чтобы рассчитать условную вероятность P(есть|счастье) считаем количество пар 'счастье есть' и делим на количество в тексте слова 'счастье':

P(есть|счастье) = C(счастье есть)/С(счастье)


В результате, если мы посчитаем все пары слов в некотором тексте, мы сможем вычислить вероятность произвольной фразы. А если мы сможем вычислить вероятность фразы, мы можем подобрать наиболее «правдоподобные» сочетания слов в данном тексте для автоматической генерации. Кстати, эти пары слов называются биграммы, а набор рассчитанных вероятностей — биграммной моделью.

Хочу отметить, что для нашего алгоритма мы используем не биграммы, а триграммы. Отличие в том, что условная вероятность слова определяется не по одному, а по двум предыдущим словам. То есть вместо P(удовольствие|счастье) мы будем вычислять P(удовольствие|счастье есть). Формула вычисления подобна формуле для биграмм:

P(удовольствие|счастье есть) = C(счастье есть удовольствие)/С(счастье есть)


Итак, генерацию текста можно провести следующим образом. Мы будем формировать отдельно каждое предложение, выполняя следующие шаги:

* выбираем слово, наиболее вероятное для начала предложения;
* подбираем наиболее вероятное слово-продолжение в зависимости от двух предыдущих слов;
* повторяем предыдущий шаг до тех пор, пока не встретим символ конца предложения.

Реализация программы

Сначала необходимо подготовить текст, с помощью которого будет обучаться модель.

Из данного текста выделим необходимую последовательность слов.

def gen_lines(corpus):
    data = open(corpus)
    for line in data:
        yield line

def gen_tokens(lines):
    for line in lines:
        for token in r_alphabet.findall(line):
            yield token

Получившийся генератор tokens будет выдавать «очищенную» последовательность слов и знаков препинания. Однако, простая последовательность нам не интересна. Нам интересны тройки токенов (под токеном здесь понимается слово или знак препинания, т.е. некие атомарные элементы текста). Для этого добавим функцию get_trigrams, которая возвращает три токена по порядку.

def gen_trigrams(tokens):
    t0, t1 = '$', '$'
    for t2 in tokens:
        yield t0, t1, t2
        if t2 in '.!?':
            yield t1, t2, '$'
            yield t2, '$','$'
            t0, t1 = '$', '$'
        else:
            t0, t1 = t1, t2

lines = gen_lines('tolstoy.txt')
tokens = gen_tokens(lines)
trigrams = gen_trigrams(tokens)

// Пример переделать
Метод gen_trigrams требует пояснения. Символы '$' используются для маркировки начала предложения. В дальнейшем, это делает проще выбор первого слова генерируемой фразы. В целом, метод действует следующим образом: он возвращает три подряд идущих токена, на каждой итерации сдвигаясь на один токен:

На входе: 
'Счастье есть удовольствие без раскаяния'

На выходе: 
итерация токены
0: $ $ счастье
1: $ счастье есть
2: счастье есть удовольствие
3: есть удовольствие без


Далее, рассчитаем триграммную модель:

def train(corpus):
    lines = gen_lines(corpus)
    tokens = gen_tokens(lines)
    trigrams = gen_trigrams(tokens)

    bi, tri = defaultdict(lambda: 0.0), defaultdict(lambda: 0.0)

    for t0, t1, t2 in trigrams:
        bi[t0, t1] += 1
        tri[t0, t1, t2] += 1

    model = {}
    for (t0, t1, t2), freq in tri.iteritems():
        if (t0, t1) in model:
            model[t0, t1].append((t2, freq/bi[t0, t1]))
        else:
            model[t0, t1] = [(t2, freq/bi[t0, t1])]
    return model

model = train('model.txt')

В первой части этого метода мы задаем генераторы. Далее, рассчитываем биграммы и триграммы (фактически, мы считаем количество одинаковых пар и троек слов в тексте). Далее, вычисляем вероятность слова в зависимости от двух предыдущих, помещая данное слово и его вероятность в словарь. Необходимо учесть, что текст должен быть сгенерирован на заданную тему. Для этого функция uni_not_rand добавляет из модели слова по следующему правилу: 

1) Попытаться получить из модели слова, которые могут следовать за словами, которые идут за токенами key1 и key2.
2) Если это не получится, выполнить функцию unirand, описанную выше. В этом случае в предложение добавится слово, наиболее вероятное в данном контексте.
3) Если на шаге 1 удается получить необходимые слова, то выбираем слово с максимальным весом из тех, которые надо вывести.
4) Для каждого w и freq (слово и его частота в корпусе) проверяем, не совпадает ли оно с требуемым.
5) Если совпадение найдено, то уменьшаем вес выведенного слова в 2 раза.
6) После этого возвращаем слово, которое было получено на предыдущем шаге.
Алгоритм представлен на рис. 3

// Вставить рисунок

def uni_not_rand(words, model, key1, key2):
	try:
		seq = model[key1, key2]
		max_fig = max(words.items(), key=itemgetter(1))[0]
		for w, freq in seq:
			if w == max_fig:
				words.update({max_fig: words[max_fig]/2})
				return w
		return unirand(model[key1, key2])
	except KeyError:
		return unirand(model["для", "этого"])

Функция, которая возвращает предложение описана ниже.

def generate_sentence(words, model):
	phrase = []
	t0, t1 = '$', '$'
	while 1:
		t0, t1 = t1, uni_not_rand(words=words, model=model, key1=t0, key2=t1)
		if t1 == '$': break
		phrase += [t1]
	return phrase

Суть метода в том, что мы последовательно выбираем наиболее вероятные слова или знаки препинания до тех пор, пока не встречаем признак начала следующей фразы (символа $). Первое слово выбирается как наиболее вероятное для начала предложения из набора model['$', '$'].

Здесь необходимо отметить важный момент. Словарь model для каждой пары слов содержит список пар (слово, вероятность). Нам же необходимо выбрать из этого набора только одно слово. Вариант «в лоб» — выбрать слово с максимальной вероятностью. Но тогда все сгенерированные фразы были бы похожи друг на друга. Более подходящий способ способ — выбирать слова с некой хаотичностью, которая бы зависела от вероятности слова (мы же не хотим чтобы наши фразы состояли из редких сочетаний). Это и делает метод unirand, который возвращает случайное слово с вероятностью, равной вероятности данного слова в зависимости от двух предыдущих.

Генерация предложений на заданную тему осуществляется ...

Алгоритм представлен на рис. 4
// Вставить алгоритм генерации

После генерации каждого нового предложения выполняется сравнение слов, использованных в нем со словарем слов с вычисленными весами, для того, чтобы убедиться в соответствии темы предложения заданной.

Пример кода генерации:

words_of_words_initial = [ x for x in open('hands_desc.txt').readlines() ]
words_of_words = []
for w in words_of_words_initial:
	words_with_weight = {}
	w_splitted = [ x for x in re.split('[; ]', w) if x ]
	print(w_splitted)
	for w1 in w_splitted:
		words_with_weight.update({w1.strip(): 1})
	words_of_words.append(words_with_weight)

for words in words_of_words:
	sent = generate_sentence(words, model)
	cnt = 0
	while compare_sentence(sent, list(words.keys())) < 0.5 and cnt < 10:
		sent = generate_sentence(words, model)
		cnt += 1
		
	print(sent, words)	

Заключение

Список литературы

# TODO: Оформить по госту!
1. Романовский И.В. Дискретный анализ: Учебное пособие для студентов, 3-е изд.  — СПб: Невский Диалект; БХВ Петербург, 2003.
2. Таха, Хэмди А. Введение в исследование операций, 6-е изд. — М.: Издательский дом «Вильямс», 2001.
3. Вернер М. Основы кодирования. Учебник для ВУЗов. — М.: Техносфера, 2004.
4. Беляев А., Гаврилов М., Масальских А., Медвинский М. Марковские процессы, 2004.